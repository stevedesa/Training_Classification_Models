{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Understanding and Training Classification Models\n",
    "## Regardless of the level you are in this course, you are NOT allowed to use import tools from the scipy or sci-kit learn.  The only imports allowed on this project are pandas, numpy, and matplotlib (or plotly/seaborn if you prefer).\n",
    "\n",
    "### Name:\n",
    "### Course Level:\n",
    "\n",
    "## Due: Friday, Feb. 28, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction:**\n",
    "* In this project, we explore the application of classification using: a) Logistic Regression and b) Multiclass Logistic Regression. The project will be broken into sections in which students registered for CSC 448 will complete the first section, CSC 548 students will complete the first and second section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objectives:**\n",
    "* The objective of this project is to implement different classification models to analyze real-world datasets, understand the relationship between variables, and perform classifications.  Additionally, students will gain experience understanding optimization techniques, linear algebra, and subspace learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The first problem we aim to analyze is a a binary classifier to determine the likelihood of a students academic success based on 36 different features (i.e., $\\textbf{x} \\in \\mathbb{R}^{36}$).  Download the dataset [Here](https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success) along with a description of what the features represent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem A (65pts)**\n",
    "\n",
    "1 (5pts). The first thing you'll need to do is read the data in, and then decide how best to handle data of different types, e.g., the target variables in this case are catgorical (dropout, enrolled, and graduate).  <u>Because we're interested in a binary classification</u>, let's just treat the \"enrolled\" the same as \"graduate\".  This will allow for the target to be [0,1] where 0 corresponds to a dropout and 1 means they are either enrolled or graduated.\n",
    "\n",
    "* Normally, we'd do some Exploritory Data Analysis (EDA) to see what features were the best to use for predictors of success, however in the problem let's just assume they are all important and equally weighted.\n",
    "* You will also need to split the dataset into a training set, and testing set (don't worry about a validation set here).  To accomplish this, randomly select 80% of the data for training and use the remaining 20% for testing and evaluation.\n",
    "    * Note: You should first sort the data into the two classes so you can ensure you grab 80% from each class to form your training/testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data (however you prefer), and modify the target from a categorical variable to a binary target [0,1] #\n",
    "# Store the input features (x_input) in one array and the tartget features (t_target) in a separate array #\n",
    "\n",
    "\n",
    "# Split the dataset into training vs. testing (ensure propoer stratification here) #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 (10pts). Next, write a function to evaluate how well the model performs (for this we can just use classification rate as the metric - # of correct classifications).  Note this is a rate so should return the accuracy of your model:\n",
    "$$\n",
    "    CR = \\frac{\\text{Correct Classification}}{\\text{All Instances Tested}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definition here, returns the classification rate between your models predicted target and the true value #\n",
    "def ClassificationRate(m_model_params, x_input, t_target):\n",
    "\n",
    "    return CR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 (40pts). Next write a function (called LogisticRegression) to learn the model parameters using gradient descent (or stochastic gradietn descent if you choose). \n",
    "    - The function will take as input, the features (x_input), the target values (t_target), and learning rate. (only using the training set!!!!!)\n",
    "    - The function will return the model parameters.\n",
    "\n",
    "* **Hint**:  You might consider creating a \"helper\" function to compute the gradient descent for you.\n",
    "\n",
    "**Note**  \n",
    "* You should assume the model takes the form\n",
    "$$\n",
    "    y = \\sigma \\left( w_0 + \\sum_{j=1}^M x_j w_j \\right) = \\sigma (\\textbf{w}^T \\textbf{x} + w_0),\n",
    "$$\n",
    "where $j$ corresponds to the $j^{\\text{th}}$ feature, and recall\n",
    "$$\n",
    "    \\sigma (z) = \\frac{1}{1 + \\exp(-z)}.\n",
    "$$\n",
    "\n",
    "* **Note:** We derived the gradients for the weight updates as:\n",
    "$$\n",
    "    \\frac{\\partial \\ell(\\textbf{w})}{\\partial w_j} = \\sum_{n=1}^N \\left( x_j^{(n)} \\left( t^{(n)} - \\underbrace{\\frac{1}{1 + \\text{exp}(-z^{(n)})}}_{p(t^{(n)}=1|\\textbf{x}^{(n)})} \\right) \\right),\n",
    "$$\n",
    "where recall that $j$ is the $j^\\text{th}$ dimension of the $n^\\text{th}$ training sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute logistic regression model parameters #\n",
    "def LogisticRegression(t_target, x_input, l_learning_rate):\n",
    "\n",
    "\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 (10pts). Evaluate the performance of the classifier.  Similar to k-fold cross validation, to get a good idea of how well your model is performing, do an 80/20 split, and run 5-fold validation on the splits.  \n",
    "\n",
    "**Example:** Do a random 80/20 split.  Using 80% of the data, train your model and test on the 20% remaining.  Do this at least 5 times (grabbing a new random 80/20 split of the data) and compute the mean and devation for the classification accuracy.\n",
    "\n",
    "* Note: This is NOT k-fold, but it will give you a better idea about how well your model is working, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation (display the mean and deviation of the classification rate) #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem B (25pts)**\n",
    "1 (5pts). Let's up the complexity a bit.  Here, we're interested in understanding how multiclass classification works.  Let's investigate a 3-class problem by looking at the IRIS dataset (download [Here](https://archive.ics.uci.edu/dataset/53/iris)).  This dataset contains 150 instances relating three different IRIS types (4 different features for each of the 150 samples).  Similar to the above problem, our first step is to download the data and get the data into our notebook.  Let's generate three different scatter plots to see what we're dealing with:\n",
    "\n",
    "- One looking at Sepial Length vs. Sepial Width vs. Petal Length (i.e., the first three features)\n",
    "- One looking at Sepial Length vs. Sepial Width vs. Petal Width\n",
    "- One looking at Sepial Width vs. Petal Length vs. Petal Width\n",
    "\n",
    "* **Note:** These should be on a single figure with three different subplots for each.  The goal is to be able to investigate the class separability based on the different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D scatter for the three different plots outlined above #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 (15pts). Assuming a linear regression model, with a softmax output, write a function called LogRegMultClass(t_target, x_input, l_learning_rate) that returns the model paramters.\n",
    "\n",
    "* **Note:** Similar to the above, you should use an 80/20 split to divide the data into a training vs. testing set.  You might want to think about how you can use your Gradient descent helper function in Problem A to cmpute the gradients for this problem as well.\n",
    "* **Hint:** You might consider writing a quick helper function to perform the softmax output, and recall:\n",
    "$$\n",
    "            \\text{softmax}(z_1, \\dots, z_k) = \\left[\n",
    "            \\begin{array}{c}\n",
    "                \\frac{\\exp{(z_1)}}{\\sum_{j=1}^k \\exp{(z_j)}}\\\\\n",
    "                \\vdots \\\\\n",
    "                \\frac{\\exp{(z_k)}}{\\sum_{j=1}^k \\exp{(z_j)}}\\\\\n",
    "            \\end{array}\n",
    "            \\right]\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform multiclass logistic regression #\n",
    "def LogRegMultClass(t_target, x_input, l_learning_rate):\n",
    "\n",
    "\n",
    "    return w\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 (5pts). To investigate the how well the classifier works, similar to Problem A.4, evaluate the model using 5-fold validation and present the mean and deviation for the classification rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Performance #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem C (10pts)**\n",
    "* In Problem B, you investigated the IRIS dataset.  In Problem C, we'll look at constructing the decision boundary between setosa vs. virginica and versicolor (just assume that virginica and versicolor belong to the same class - e.g., this is a binary classification problem).\n",
    "* Furhtermore, remove all features from the dataset except sepal length and sepal width (this will result in a 2-dimensional feature space)\n",
    "* Using our logistic regression model, learn the model parameters as in Problem A, and display a plot showing the two different classes (in different colors) along with the line illustrating this decision boundary between the two classes. [Example of what we're looking for can be found here](https://drive.google.com/file/d/1_OpNJGUHbwiqQhNlIj_2H2Htib-MJcfi/view?usp=share_link)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn and plot the decision boundary for the IRIS dataset #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSC 548 Only!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem D (10pts)**\n",
    "* In class we presented claimed that through some mathematical manipulation, we could arrive at a much easier form for the log-likelihood loss for logistic regression models, show that this is indeed the case.\n",
    "\n",
    "* Demonstrate each step of the process to go from here\n",
    "$$\n",
    "        \\begin{array}{lll}\n",
    "            \\ell(\\textbf{w}) & = & - \\displaystyle{ \\sum_{n=1}^N  t^{(n)} \\log \\left( 1 - p(t^{(n)}=0|\\textbf{x}^{(n)};\\textbf{w}) \\right)}\\\\\n",
    "            & & \\;\\;\\;\\;\\; - \\displaystyle{ \\sum_{n=1}^N (1 - t^{(n)}) \\log \\left( p(t^{(n)}=0|\\textbf{x}^{(n)};\\textbf{w}) \\right)    }\\\\\n",
    "        \\end{array}\n",
    "$$\n",
    "to arrive here:\n",
    "$$\n",
    "    \\ell(\\textbf{w}) = \\sum_{n=1}^N \\log \\left( 1 + \\text{exp}(-z^{(n)}) \\right) + \\sum_{n=1}^N t^{(n)} z^{(n)}\n",
    "$$\n",
    "\n",
    "**Recall:**\n",
    "$$\n",
    "    p(t^{(n)} = 1|\\textbf{x}^{(n)};\\textbf{w}) = \\frac{1}{1 + \\text{exp}(-z)},\n",
    "$$\n",
    "and\n",
    "$$\n",
    "    p(t^{(n)} = 0|\\textbf{x}^{(n)};\\textbf{w}) = \\frac{\\text{exp}(-z)}{1 + \\text{exp}(-z)},\n",
    "$$\n",
    "where $z = \\textbf{w}^T \\textbf{x} + w_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ins>*Solution:*</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
